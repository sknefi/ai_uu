# UÄenie s uÄiteÄ¾om (Supervised Learning)

## Ãšvod

Po inteligentnÃ½ch agentoch Äasto chceme, aby sa vedeli uÄiÅ¥. UÄenie mÃ´Å¾e maÅ¥ viacero podÃ´b, podÄ¾a toho, koÄ¾ko informÃ¡ciÃ­ mÃ¡me k dispozÃ­cii:

- **Bez informÃ¡ciÃ­** â€“ agent sa uÄÃ­ sÃ¡m z prostredia
- **S evaluÃ¡ciou** â€“ dostÃ¡va spÃ¤tnÃº vÃ¤zbu, ako dobre sa rozhodol
- **S uÄiteÄ¾om** â€“ dostÃ¡va vstupy a sprÃ¡vne vÃ½stupy pre uÄenie

V tejto kapitole sa zameriame na **uÄenie s uÄiteÄ¾om**, najmÃ¤ pomocou **neuronovÃ½ch sietÃ­**.

---

## VstupnÃ© dÃ¡ta

NeuronovÃ¡ sieÅ¥ je matematickÃ¡ funkcia, ktorÃ¡ pre vstupy vracia vÃ½stupy. Pri uÄenÃ­ s uÄiteÄ¾om pouÅ¾Ã­vame **trÃ©novaciu mnoÅ¾inu dÃ¡t**, ktorÃ¡ obsahuje dvojice:

- `x` â€“ vstup (napr. obraz)
- `y` â€“ vÃ½stup (napr. identifikÃ¡cia objektu)

CieÄ¾om uÄenia je nÃ¡jsÅ¥ takÃ© vÃ¡hy a prahy, aby vÃ½stupy siete Äo najviac zodpovedali reÃ¡lnym dÃ¡tam. ZÃ¡roveÅˆ nÃ¡s zaujÃ­ma, ako dobre vie sieÅ¥ **generalizovaÅ¥** â€“ teda ako si poradÃ­ s novÃ½mi dÃ¡tami, ktorÃ© predtÃ½m nevidela.

---

## Chyba aproximÃ¡cie

Pri uÄenÃ­ chceme minimalizovaÅ¥ **chybu aproximÃ¡cie** â€“ rozdiel medzi vÃ½stupom siete a oÄakÃ¡vanÃ½m vÃ½stupom.

NajÄastejÅ¡ie sa pouÅ¾Ã­va metrika **mean squared error** (MSE), teda priemer Å¡tvorcov rozdielov medzi vÃ½stupmi siete a poÅ¾adovanÃ½mi hodnotami.

---

## Algoritmus Backpropagation

NajznÃ¡mejÅ¡Ã­ algoritmus uÄenia viacvrstvovÃ½ch sietÃ­ je **backpropagation**, ktorÃ½ funguje nasledovne:

1. SpoÄÃ­tame vÃ½stupy siete a chybu na trÃ©novacej mnoÅ¾ine.
2. Chyba je funkcia vÅ¡etkÃ½ch vÃ¡h siete.
3. SpoÄÃ­tame **gradient** â€“ smer najvÃ¤ÄÅ¡ieho klesania chyby.
4. Postupne upravujeme vÃ¡hy v smere tohto gradientu.
5. Tento postup opakujeme, kÃ½m chyba neklesne dostatoÄne.

CieÄ¾om je nÃ¡jsÅ¥ **globÃ¡lne minimum** v krajine chybovej funkcie.

---

## AlternatÃ­vne algoritmy

Existuje mnoho alternatÃ­vnych algoritmov k backpropagation:

- **Silva-Almeida** â€“ adaptÃ­vna uÄebnÃ¡ rÃ½chlosÅ¥
- **Delta-bar-delta** â€“ opatrnejÅ¡ia zmena rÃ½chlosti uÄenia
- **Rprop** â€“ zameriava sa na rÃ½chle prekonÃ¡vanie plochÃ½ch oblastÃ­ chyby
- **Quickprop** â€“ berie do Ãºvahy aj zakrivenie chyby (druhÃ© derivÃ¡cie)
- **QRprop** â€“ kombinuje Rprop a Quickprop

---

## AplikÃ¡cie a limity neuronovÃ½ch sietÃ­

NeuronovÃ© siete sÃº vhodnÃ© pre Ãºlohy, kde potrebujeme **generalizÃ¡ciu**. SÃº vÅ¡ak aj limitovanÃ©:

âœ… VhodnÃ© na rozpoznÃ¡vanie vzorov, predikcie, klasifikÃ¡ciu

âŒ MÃ´Å¾u sa **preuÄiÅ¥** (overfitting)

âŒ SÃº **nÃ¡roÄnÃ© na dÃ¡ta a vÃ½poÄty**

âŒ SÃº **Å¥aÅ¾ko interpretovateÄ¾nÃ©** â€“ Äierna skrinka

---

## Nastavenie siete

DÃ´leÅ¾itÃ© je zvoliÅ¥ vhodnÃº **topolÃ³giu siete** â€“ poÄet vrstiev a neurÃ³nov.

- ZaÄni s 1 skrytou vrstvou
- Ak nefunguje dobre, pridaj neurÃ³ny alebo vrstvy
- Pri veÄ¾a neurÃ³noch hrozÃ­ preuÄenie (overfitting)
- VÃ¡hy sa zvyÄajne inicializujÃº nÃ¡hodne z intervalu `[-a, a]`

---

## Ako zlepÅ¡iÅ¥ uÄenie

- **DobrÃ¡ topolÃ³gia siete**
- **VhodnÃ¡ uÄebnÃ¡ rÃ½chlosÅ¥**
- **Moment setrvaÄnosti** â€“ pamÃ¤tÃ¡ si smer uÄenia
- **NÃ¡hodnÃ½ Å¡um** â€“ zniÅ¾uje riziko uviaznutia v lokÃ¡lnom minime
- **RozÅ¡Ã­renie dÃ¡t** â€“ viac dÃ¡t, lepÅ¡ie uÄenie
- **Early stopping** â€“ ukonÄenie uÄenia skÃ´r, ako nastane overfitting

---

## Overfitting vs Underfitting

- **Overfitting** â€“ sieÅ¥ sa nauÄila presne trÃ©novacie dÃ¡ta, ale nefunguje na novÃ©
- **Underfitting** â€“ sieÅ¥ sa nenauÄila ani trÃ©novacie dÃ¡ta
- RieÅ¡enÃ­m je **early stopping** a **testovacia mnoÅ¾ina**

---

## RegularizÃ¡cia a Dropout

- **L1/L2 regularizÃ¡cia** â€“ trest za prÃ­liÅ¡ veÄ¾kÃ© vÃ¡hy
- **Dropout** â€“ poÄas uÄenia nÃ¡hodne vypÃ­name niektorÃ© neurÃ³ny

---

## InÃ© formy uÄenia s uÄiteÄ¾om

Okrem neuronovÃ½ch sietÃ­ existujÃº aj inÃ© metÃ³dy:

- **LineÃ¡rna regresia**
- **Random Forests**
- **Support Vector Machines (SVM)**

---

Ak chceÅ¡ k niektorej Äasti obrÃ¡zok alebo doplniÅ¥ kÃ³d, daj vedieÅ¥! ğŸ’¡
